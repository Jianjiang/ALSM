**********
Chapter 17 -- Analysis of Factor Level Means
================================================================================
**********

```{r set-global-opts, include=FALSE}
opts_chunk$set(cache=TRUE, tidy=FALSE)
```

Load the data sets
----------------------------------------------------------------------------

```{r}
env <- new.env()
load("../data.rda", envir = env)
```


TABLE 17.1
----------------------------------------------------------------------------

#### Summary of Results--Kenton Food Company Example

Ignored due to lack of pedagogical benefit. See Chapter 16 for details.



Input Rust Inhibitor Data
----------------------------------------------------------------------------

```{r}
df <- get("CH17TA02", envir = env)
names(df) <- c("y", "x1", "x2")
fit <- lm(y ~ factor(x1), df)
```


TABLE 17.2     (p 735)
----------------------------------------------------------------------------

#### Data and Analysis of Variance Results--Rust Inhibitor Example

The last two steps demonstrate how to use the information to extract some specific values, but of course this information is already displayed in the ANOVA table, as the p-value tells us the result of the hypothesis of the F-test. 

```{r}
addmargins(xtabs(y ~ x2 + x1, df), 1, mean)
cat("Ybar.. = ", mean(df$y))
anova(fit)
anova(fit)[1, 3] / anova(fit)[2,3]  # F* = MSTR / MSE = 866.12
qf(0.95, 3, 36)                     # F(0.95, 3, 36) = 2.87 < F*
```


Input Kenton Food Company Data
----------------------------------------------------------------------------

```{r}
df <- get("CH16TA01", envir = env)
names(df) <- c("y", "x1", "x2")
```


FIGURE 17.1     (p 736)
----------------------------------------------------------------------------

#### Line Plot of Estimated Factor Level Means--Kenton Food Company Example

I don't know of a line plot function, but a makeshift way to generate a similar graph is to use `stripchart` (*graphics*).

```{r}
with(df, stripchart(tapply(y, x1, mean), pch = 20, ylim = c(1, 2), xlim = c(12, 30), xlab = "Cases Sold"))
with(df, text(tapply(y, x1, mean), rep(1, 4), labels = seq(4), pos = 3))
```


FIGURE 17.2 and FIGURE 17.3    (p 736, 739)
----------------------------------------------------------------------------

#### Bar Graph and Main Effects Plot of Estimated Factor Level Means--Kenton Food Company Example
#### Bar-Interval Graph and Interval Plot--Kenton Food Company Example

```{r plots, fig.width=12, fig.height=6}
# The factor means to be plotted
means <- with(df, tapply(y, x1, mean))

# The radius portion of the 95% confidence interval for each factor
diff  <- with(df, qt(0.975, 15) * sqrt(10.55 / tapply(y, x1, length)))

par(mfrow = c(1, 2))
barplot(means, xlab = "Design", ylab = "Cases Sold")
title("(a) Bar Graph")

plot(means, type = "o", pch = 19, ylim = c(0, 30), xlab = "Design", ylab = "Cases Sold")
abline(h = mean(df$y), lty = 2, col = "gray40", lwd = 2)
title(main = "(b) Main Effects Plot")

bar <- barplot(means, xlab = "Design", ylab = "Cases Sold", ylim = c(0, 32))
arrows(bar, means+diff, bar, means-diff, angle = 90, code = 3)
title("(a) Bar-Interval Graph")

plot(means, pch = 19, ylim = c(0, 30), xlim = c(.5, 4.5), xlab = "Design", ylab = "Cases Sold")
arrows(seq(4), means+diff, seq(4), means-diff, angle = 90, code = 3)
abline(h = mean(df$y), lty = 2, col = "gray40", lwd = 2)
title("(b) Interval Plot")
```



EXAMPLE     (p 739-41)
----------------------------------------------------------------------------

#### Inferences For Difference Between Two Factor Level Means

```{r}
means <- with(df, tapply(y, x1, mean))         # Defined from earlier
D = means['3'] - means['4']                    # (17.11)
se = sqrt(10.55 * (1/4 + 1/5))                 # (17.14)
c("lwr" = D, "upr" = D) +                      # (17.16) +/- "t(0.975, 15) x se"
  c(-qt(0.975,15) * se, qt(0.975, 15) * se)
cat('t* =', D / se)                            # (17.18)
```



EXAMPLE     (p 741-43)
----------------------------------------------------------------------------

#### Inferences For Contrast of Factor Level Means

```{r}
lengths <- with(df, tapply(y, x1, length))
means   <- with(df, tapply(y, x1, mean))
cont    <- c(1/2, 1/2, -1/2, -1/2)              # contrasts
L  <- sum(cont * means)                         # (17.20)
se <- sqrt(10.55 * sum(cont^2 / lengths))       # (17.22)
c('lwr' = L, 'upr' = L) +                       # (17.24)
  c(-qt(.975, 15)*se, qt(.975, 15)*se)
cat('t* =', L / se)                             # (17.26)
```




Input Rust Inhibitor Data
----------------------------------------------------------------------------

```{r}
df <- get("CH17TA02", envir = env)
names(df) <- c("y", "x1", "x2")
```


FIGURE 17.4 and TABLE 17.3     (p 748-9)
----------------------------------------------------------------------------

#### Simultaneous Confidence Intervals and Tests for Pairwise Differences Using the Tukey Procedure--Rust Inhibitor Example
#### Paired Comparison Plot--Rust Inhibitor Example

The `TukeyHSD` method (*stats*) performs a test for pairwise differences and contains a method for plotting those differences. It is not the same as this figure, but it is just as informative. Note that `TukeyHSD` requires an `aov` object. It will not take an `lm` object.

Here `TukeyHSD` presents the appropriate p-values. If you want to test on the *q** distribution, you can look into `qtukey`. Moreover, note the differences presented. The order matters (for directionality), but the comparisons result in the same conclusions. 

This same approach can be used for "Example 2--Unequal Sample Sizes" (p 750). Assuming the Kenton Food data were loaded, the following should suffice. It will be presented in the Scheffe example below.

> `TukeyHSD(aov(y ~ factor(x1), df), conf.level = .90)`

```{r}
fit <- aov(y ~ factor(x1), df)
TukeyHSD(fit)
plot(TukeyHSD(fit))  # Diff 4-1 contains 0
```



Input Kenton Food Data
----------------------------------------------------------------------------

```{r}
df <- get("CH16TA01", envir = env)
names(df) <- c("y", "x1", "x2")
```


EXAMPLE     (p 753-5)
----------------------------------------------------------------------------

### Scheffe Multiple Comparison Procedure

As far as we know, R has no methods for this multiple comparison procedure. Therefore, we've created a function that takes a contrasts matrix

```{r}
Scheffe <- function(formula, data, cont, conf.level = 0.90, MSE) 
{
    f  <- function(contrast, mean)   {contrast %*% mean}                # (17.41)
    h  <- function(contrast, n, MSE) {sqrt(MSE * sum(contrast^2 / n))}  # (17.42)
    
    df <- model.frame(formula, data)
    y  <- df[, 1]
    x  <- df[, 2]
    r  <- nrow(cont)

    means <- tapply(y, x, mean)
    L     <- apply(cont, 2, f, mean = means)
    se    <- apply(cont, 2, h, n = tapply(y, x, length), MSE = MSE)
    S     <- sqrt((r-1) * qf(conf.level, r-1, nrow(df)-r))               # (17.43a)
    L + S*cbind('lwr' = -se, 'upr' = se)                                 # (17.43)
}
cont <- matrix(c(0.5,  0.5, -0.5, -0.5,
                 0.5, -0.5,  0.5, -0.5,
                   1,   -1,    0,    0,
                   0,    0,    1,   -1), nrow = 4)
Scheffe(y ~ x1, df, cont = cont, conf.level = 0.90, MSE = 10.55)

TukeyHSD(aov(y ~ factor(x1), df), conf.level = 0.90)                 # For Example 2--Unequal Sample Sizes (p 750)
```



EXAMPLE     (p 756-7)
----------------------------------------------------------------------------

#### Bonferroni Multiple Comparison Procedure

```{r}
Bonferroni <- function(formula, data, cont, conf.level = 0.975, MSE) {
    f  <- function(contrast, mean)   {contrast %*% mean}                # (17.41)
    h  <- function(contrast, n, MSE) {sqrt(MSE * sum(contrast^2 / n))}  # (17.42)
    
    df <- model.frame(formula, data)
    y  <- df[, 1]
    x  <- df[, 2]
    r  <- nrow(cont)
    g  <- ncol(cont)

    means <- tapply(y, x, mean)
    L     <- apply(cont, 2, f, mean = means)
    se    <- apply(cont, 2, h, n = tapply(y, x, length), MSE = MSE)
    B     <- qt(1 - (1 - conf.level)/(2*g), nrow(df)-r)                  # (17.46a)
    L + B*cbind('lwr' = -se, 'upr' = se)                                 # (17.46)
}

# Define contrast matrix
Bonferroni(y ~ x1, df, cont[, 1:2], conf.level = 0.975, MSE = 10.55)
Scheffe(y ~ x1, df, cont = cont[, 1:2], conf.level = 0.90, MSE = 10.55)  # Compare with Scheffe
Bonferroni(y ~ x1, df, cont, conf.level = 0.975, MSE = 10.55)  # Compare to Scheffe example above
Scheffe(y ~ x1, df, cont = cont, conf.level = 0.90, MSE = 10.55)  # From previous example

```



FIGURE 17.5     (p 759)
----------------------------------------------------------------------------

#### Analysis of Means Plot--Kenton Food Company Example

We cannot find any ANOM methods in R, but since the computations are straightforward, both the values of interest and graphic could be designed from the following code.

```{r}
r = 4
u = sum(means) / r                          # (17.48a)
n = with(df, tapply(y, x1, length))         # lengths ni
s = vector("numeric", r)
for(i in seq(r))
  s[i] = (10.55/n[i]) * ((r-1)/r)^2 + (10.55/r^2) * sum(1/n[-i])  # (17.49)

plot(x = seq(r), means, pch = 20, ylim = c(10, 30), xlab = "Levels of Design", ylab = "Mean", xaxt = 'n')
axis(1, seq(4))
segments(seq(4), u, seq(4), means)
lines(seq(1, 4.5, 0.5), rep(u+s, each = 2), type = "S")
lines(seq(1, 4.5, 0.5), rep(u-s, each = 2), type = "S")
abline(h = u)
```



Input Piecework Trainees Data
----------------------------------------------------------------------------

```{r}
df <- get("CH17TA04", envir = env)
names(df) <- c("Y", "X1", "X2")
df <- transform(df, X1 = factor(X1, labels = c(' 6 hours', ' 8 hours', '10 hours', '12 hours')))
```


TABLE 17.4 and FIGURE 17.6    (p 762-3)
----------------------------------------------------------------------------

#### Data--Piecework Trainees Example
#### Computer Output--Piecework Trainees Example

To get homogeneous subsets, one could use `multcompTs` (*multcompView*), and feed it the respective p-value matrix from `TukeyHSD`. This results in a matrix of {-1, 0, 1} values, where -1 indicates they are significantly different, 1 is the base (comparing a level to itself), and 0 is no significant difference. This may be more useful in locating those differences in large number of cases. However, since it is entirely based on the p-values of `TukeyHSD`, one could also (1) extract the non-significant p-values or plot them to maybe visually inspect those that include 0 in their confidence intervals. 

```{r}
fit <- aov(Y ~ X1, df)
addmargins(xtabs(Y ~ X1 + X2, df, sparse = TRUE), 2, list(list('Mean' = mean, 'SD' = sd)))
model.tables(fit, "means")                      # Alternative way to see table of means
summary(fit)                                    # ANOVA summary
summary.lm(fit)                                 # Coefficients are mean difference from "6 hours" mean
TukeyHSD(fit)                                   # 1st 3 'diff' match lm model coefficients
plot(TukeyHSD(fit))
cat("q* =", qtukey(0.95, 4, 24))
diff(with(df, tapply(Y, X1, mean)))             # Diminishing returns to training (p 764)
```


FIGURE 17.7 and TABLE 17.5    (p 764-5)
----------------------------------------------------------------------------

#### Scatter plot and Fitted Quadratic Response Function--Piecework Trainees Example
#### Illustration of Data for Regression Analysis--Piecework Trainees Example

Since we're treating `X1` as a factor, we manually specify it as a vector below. The mean can been taken as this vector mean (`= 9`). 

```{r}
x1 <- rep(seq(6, 12, 2), each = 7)                  # numeric encoding of X1
xx <- seq(min(x1), max(x1), len = 200)              # For curve plotting below
df <- transform(df, x = c(scale(x1, TRUE, FALSE)))  # Center X1
df <- transform(df, X = x1)                         # Store the numeric encoding
(df <- transform(df, xsq = x*x))                    # TABLE 17.5

plot(df$Y ~ x1, pch = 20, ylim = c(30, 65), xlab = "Hours of Training", ylab = "Number of Acceptable Units")
lines(xx, coef(lm(Y ~ X + I(X^2), df)) %*% rbind(1, xx, xx^2))  # Uses (17.51)
```



TABLE 17.6     (p 765)
----------------------------------------------------------------------------

#### Analysis of Variance--Piecework Trainees Example

Recall that R `anova` will output a breakdown of the Regression SS. Simply aggregating the components appropriately produces the book results.

`1764.35 + 43.75 = 1808.1`

Divide that by the degrees of freedom (2) will match the Regression MS. 

```{r}
fit <- lm(Y ~ x + xsq, df)  # (17.52)

anova(fit)                  # (a) Regression Model (17.51)
anova(lm(Y ~ X1, df))       # (b) ANOVA (17.50)
anova(fit, lm(Y ~ X1, df))  # (c) ANOVA for Lack of Fit
```
